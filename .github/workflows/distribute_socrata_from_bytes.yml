name: ðŸ“¬ Distribute Datasets
run-name: "ðŸ“¬ Distribute Datasets: ${{ inputs.product }}-${{ inputs.version }}-${{ inputs.datasets }}-${{ inputs.destination_tags }}-${{ inputs.destination_types }}"

on:
  workflow_dispatch:
    inputs:
      product:
        description: "Name of the product"
        type: string
        required: true
      version:
        description: "Version to push"
        type: string
        required: true
      source:
        description: "Source ID (e.g. bytes)"
        type: string
        required: true
      datasets:
        description: "Comma-separated list of datasets to push"
        type: string
        required: false
      destination_tags:
        description: "Comma-separated list of destination tags to filter for."
        type: string
        required: false
      destination_types:
        description: "Comma-separated list of destination types to filter for."
        type: string
        required: false
      publish:
        description: "Publish the Socrata Revision? Or leave it open."
        type: boolean
        default: false
        required: false
      metadata_only:
        description: "Only push metadata (including attachments)."
        type: boolean
        default: false
        required: false
      validate_dataset_files:
        description: "Validate assembled dataset files?"
        type: boolean
        default: false
        required: false
      # MD_REPO_BRANCH:
      #   description: "(Override) Product-Metadata branch/commit/ref"
      #   type: string
      #   default: "main"
      dry_run:
        description: "Perform a dry run (will just list destinations)."
        type: boolean
        default: false
        required: false
jobs:
  publish:
    runs-on: ubuntu-22.04
    defaults:
      run:
        shell: bash
    container:
      image: nycplanning/build-base:latest
    env:
      PUBLISHING_BUCKET: edm-publishing
      RECIPES_BUCKET: edm-recipes
      PRODUCT_METADATA_REPO_PATH: product-metadata
      _TYPER_STANDARD_TRACEBACK: 1
    steps:
      - uses: actions/checkout@v4

      - uses: actions/checkout@v4
        with:
          repository: NYCPlanning/product-metadata
          path: product-metadata
          # ref: ${{ inputs.MD_REPO_BRANCH }}


      - name: Load Secrets
        uses: 1password/load-secrets-action@v1
        with:
          export-env: true
        env:
          OP_SERVICE_ACCOUNT_TOKEN: ${{ secrets.OP_SERVICE_ACCOUNT_TOKEN }}
          AWS_S3_ENDPOINT: "op://Data Engineering/DO_keys/AWS_S3_ENDPOINT"
          AWS_SECRET_ACCESS_KEY: "op://Data Engineering/DO_keys/AWS_SECRET_ACCESS_KEY"
          AWS_ACCESS_KEY_ID: "op://Data Engineering/DO_keys/AWS_ACCESS_KEY_ID"
          SOCRATA_USER: "op://Data Engineering/DCP_OpenData/username"
          SOCRATA_PASSWORD: "op://Data Engineering/DCP_OpenData/password"


      - name: Finish container setup
        run: ./bash/docker_container_setup.sh

      - name: Distribute to Destinations
        run: |
          python3 -m dcpy lifecycle scripts package_and_distribute product \
            ${{ inputs.product }} \
            ${{ inputs.version }} \
            ${{ inputs.source }} \
            $(if [ "${{ inputs.datasets }}" != "" ]; then for ds in $(echo ${{ inputs.datasets }} | tr ',' ' '); do echo -d $ds; done; fi) \
            $(if [ "${{ inputs.destination_tags }}" != "" ]; then for tag in $(echo ${{ inputs.destination_tags }} | tr ',' ' '); do echo -t $tag; done; fi) \
            $(if [ "${{ inputs.destination_types }}" != "" ]; then for typ in $(echo ${{ inputs.destination_types }} | tr ',' ' '); do echo -y $typ; done; fi) \
            $(if [ "${{ inputs.publish }}" = "true" ]; then echo "-p"; fi) \
            $(if [ "${{ inputs.metadata_only }}" = "true" ]; then echo "-m"; fi) \
            $(if [ "${{ inputs.validate_dataset_files }}" = "true" ]; then echo "-v"; fi) \
            $(if [ "${{ inputs.dry_run }}" = "true" ]; then echo "--dry-run"; fi)
