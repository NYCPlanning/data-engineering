name: z(TODO) Population Fact Finder - Build ACS
on:
  workflow_dispatch:
    inputs:
      data_year:
        description: 'Release year of ACS data. e.g. For 2015-2019 acs5 data, type "2019"'
        required: true
        default: '2019'
      geo_year:
        description: 'Year of geographic units. Options: "2010", "2020", or "2010_to_2020"'
        required: true
        default: '2010_to_2020'
      cache:
        description: 'Would you like to use cached download data? (yes/no)'
        required: true
        default: 'yes'

jobs:
  build:
    name: "ACS Year: ${{ github.event.inputs.data_year }} Geography: ${{ github.event.inputs.geo_year }}"
    runs-on: ubuntu-22.04
    defaults:
      run:
        shell: bash
        working-directory: ./products/factfinder
    container: 
      image: nycplanning/build-base:latest
    env:
      API_KEY: ${{ secrets.API_KEY }}
      EDM_DATA: ${{ secrets.EDM_DATA }}
      DATA_YEAR: ${{ github.event.inputs.data_year }}
      GEO_YEAR: ${{ github.event.inputs.geo_year }}
    steps:
      - uses: actions/checkout@v3

      - name: Load Secrets
        uses: 1password/load-secrets-action@v1
        with:
          export-env: true
        env:
          OP_SERVICE_ACCOUNT_TOKEN: ${{ secrets.OP_SERVICE_ACCOUNT_TOKEN }}
          AWS_S3_ENDPOINT: "op://Data Engineering/DO_keys/AWS_S3_ENDPOINT"
          AWS_SECRET_ACCESS_KEY: "op://Data Engineering/DO_keys/AWS_SECRET_ACCESS_KEY"
          AWS_ACCESS_KEY_ID: "op://Data Engineering/DO_keys/AWS_ACCESS_KEY_ID"

      - name: Run Container Setup
        working-directory: ./
        run: ./bash/docker_container_setup.sh

      - name: Cache Download - so that we are less reliant on the API
        if: github.event.inputs.cache == 'yes'
        uses: actions/cache@v2
        with:
          path: .cache/download/year=${{ github.event.inputs.data_year }}/geography=${{ github.event.inputs.geo_year }}
          key: ${{ hashFiles('factfinder/download.py') }}

      - name: run pipelines/acs
        run: |
          python3 -m pipelines.acs --year $DATA_YEAR --geography $GEO_YEAR
          python3 -m pipelines.support_geoids --geography $GEO_YEAR

      - name: send to database
        run: |
          TABLE_NAME=staging-Y$DATA_YEAR-G$GEO_YEAR
          FILE_PATH=.output/acs/year=$DATA_YEAR/geography=$GEO_YEAR/acs.csv
          cat $FILE_PATH | psql $EDM_DATA -v TABLE_NAME=$TABLE_NAME -f pipelines/create_acs.sql

      - name: upload to s3
        if: github.ref == 'refs/heads/main'
        run: |
          ACS_FILE_PATH=.output/acs/year=$DATA_YEAR/geography=$GEO_YEAR/acs.csv
          SUPPORT_GEOIDS_FILE_PATH=.output/support_geoids/geography=$GEO_YEAR/support_geoids.csv
          mc cp $ACS_FILE_PATH spaces/edm-publishing/db-factfinder/acs/year=$DATA_YEAR/geography=$GEO_YEAR/
          mc cp $SUPPORT_GEOIDS_FILE_PATH spaces/edm-publishing/db-factfinder/support_geoids/geography=$GEO_YEAR/
